# Apisix 启动

[toc]





## 架构



Apisix 的架构如下：

![apissix-arch](./assets/apissix-arch.png)



可以看出 Apisix 是基于 OpenResty 基础上实现的。其实就是 nginx + LuaJIT。在博客中 [为什么Apache APISIX 选择 NGINX+Lua 技术栈](https://apisix.apache.org/zh/blog/2021/08/25/why-apache-apisix-chose-nginx-and-lua/) 说明了为什么这么选型，总结几点：

- Lua 许多核心库都是通过 C 实现的，性能很好
- LuaJIT 优化能够把动态语言的代码在运行时编译成机器码，进而把原来的代码的性能提升一个数量级。
- Nginx + Lua ：高性能 + 灵活



归根结底，apisix 就是基于 OpenResty 的一个应用，现在 Apisix 添加 Apisix Plugin Runtime 可以支持很多语言编写插件，不仅限于 lua 脚本。





## Apisix 的启动过程



### 1. 入口

在 [apisix](https://github.com/apache/apisix) 项目中，启动的入口通过 make run 触发，makefile 中 run  target 定义如下：

```makefile
ENV_APISIX             ?= $(CURDIR)/bin/apisix
...
### run : Start the apisix server
.PHONY: run
run: runtime
	@$(call func_echo_status, "$@ -> [ Start ]")
	$(ENV_APISIX) start
	@$(call func_echo_success_status, "$@ -> [ Done ]")
```



执行的就是 `bin/apisix` shell 脚本：

- 找到 Apisix 的入口 lua 脚本 `apisix.lua` 位置
- 找到 openresty 二进制文件命令
- 找到 LuaJIT 的位置
- 最后，通过 LuaJIT 运行入口脚本 apisix.lua ，并且可以串参数进取，例如 `apisix reload`

```shell
    exec $LUAJIT_BIN $APISIX_LUA $*
```



apisix 的 lua 入口脚本在 `apisix/cli/apisix.lua`，该文件主要作用：

- 添加 apisix 相关的 lua 依赖和 c 语言的 so 动态库依赖
- 检查 openresty 、LuaJIT 环境是否就绪
- 检查 ulimit 目前设置的最大打开文件数
- 检查 nginx 中是否编译了 `apisix-nginx-module` 模块
- 执行启动命令

```lua
-- apisix/cli/apisix.lua

-- 默认的 cpath 和 path
local pkg_cpath_org = package.cpath
local pkg_path_org = package.path

local _, find_pos_end = string.find(pkg_path_org, ";", -1, true)
if not find_pos_end then
    pkg_path_org = pkg_path_org .. ";"
end

local apisix_home = "/usr/local/apisix"
local pkg_cpath = apisix_home .. "/deps/lib64/lua/5.1/?.so;"
                  .. apisix_home .. "/deps/lib/lua/5.1/?.so;"
local pkg_path_deps = apisix_home .. "/deps/share/lua/5.1/?.lua;"
local pkg_path_env = apisix_home .. "/?.lua;"

-- modify the load path to load our dependencies
-- 在默认 path 和 cpath 前边添加 apisix 依赖
package.cpath = pkg_cpath .. pkg_cpath_org
package.path  = pkg_path_deps .. pkg_path_org .. pkg_path_env

-- pass path to construct the final result
-- 检查 openresty, LuaJIT, ulimit 等参数
local env = require("apisix.cli.env")(apisix_home, pkg_cpath_org, pkg_path_org)
local ops = require("apisix.cli.ops")

-- 执行启动
ops.execute(env, arg)
```





### 2. 命令入口

apisix 支持一些不同的执行命令，其入口定义都在 `apisix/cli/ops.lua` 中，包括：

```lua
-- apisix/cli/ops.lua
local function help()
    print([[
Usage: apisix [action] <argument>

help:       print the apisix cli help message
init:       initialize the local nginx.conf
init_etcd:  initialize the data of etcd
start:      start the apisix server
stop:       stop the apisix server
quit:       stop the apisix server gracefully
restart:    restart the apisix server
reload:     reload the apisix server
test:       test the generated nginx.conf
version:    print the version of apisix
]])
end

local action = {
    help = help,
    version = version,
    init = init,
    init_etcd = etcd.init,
    start = start,
    stop = stop,
    quit = quit,
    restart = restart,
    reload = reload,
    test = test,
}

-- 命令执行入口
function _M.execute(env, arg)
    local cmd_action = arg[1]
    if not cmd_action then
        return help()
    end

    if not action[cmd_action] then
        stderr:write("invalid argument: ", cmd_action, "\n")
        return help()
    end

    -- action table 中找到对应命令执行
    action[cmd_action](env, arg[2])
end
```

下面主要分析 start 启动命令



### 3. start 命令

```lua
local function start(env, ...)
    -- 设置 apisix_home 和 删除 .customized_config_path 文件
    cleanup(env)

    if env.apisix_home then
        profile.apisix_home = env.apisix_home
    end

    -- nginx 进程启动起来是 nobody 用户运行的，无法访问 root 目录
    if env.is_root_path then
        util.die("Error: It is forbidden to run APISIX in the /root directory.\n")
    end

    -- 创建 日志 目录
    local logs_path = env.apisix_home .. "/logs"
    if not pl_path.exists(logs_path) then
        local _, err = pl_path.mkdir(logs_path)
        if err ~= nil then
            util.die("failed to mkdir ", logs_path, ", error: ", err)
        end
    elseif not pl_path.isdir(logs_path) and not pl_path.islink(logs_path) then
        util.die(logs_path, " is not directory nor symbol link")
    end

    -- 通过 /logs/nginx.pid 检查是否有已经在运行的 apisix 进程
    local pid = nil
    for i = 1, 30 do
        local running
        running, pid = check_running(env)
        if not running then
            break
        else
            sleep(0.1)
        end
    end

    -- 如果已经有运行的进程，kill 
    if pid then
        if pid <= 0 then
            print("invalid pid")
            return
        end

        local signone = 0

        local ok, err, err_no = signal.kill(pid, signone)
        if ok then
            print("the old APISIX is still running, the new one will not start")
            return
        -- no such process
        elseif err_no ~= errno.ESRCH then
            print(err)
            return
        end

        print("nginx.pid exists but there's no corresponding process with pid ", pid,
              ", the file will be overwritten")
    end

    -- 启动 apisix 
    local parser = argparse()
    parser:argument("_", "Placeholder")
    parser:option("-c --config", "location of customized config.yaml")
    -- TODO: more logs for APISIX cli could be added using this feature
    parser:flag("-v --verbose", "show init_etcd debug information")
    local args = parser:parse()

    -- 解析配置文件 config.yaml
    local customized_yaml = args["config"]
    if customized_yaml then
        local customized_yaml_path
        local idx = str_find(customized_yaml, "/")
        if idx and idx == 1 then
            customized_yaml_path = customized_yaml
        else
            local cur_dir, err = lfs.currentdir()
            if err then
                util.die("failed to get current directory")
            end
            customized_yaml_path = cur_dir .. "/" .. customized_yaml
        end

        if not util.file_exists(customized_yaml_path) then
           util.die("customized config file not exists, path: " .. customized_yaml_path)
        end

        local ok, err = util.write_file(profile:customized_yaml_index(), customized_yaml_path)
        if not ok then
            util.die("write customized config index failed, err: " .. err)
        end

        print("Use customized yaml: ", customized_yaml)
    end

    -- 重要，初始化 apisix
    init(env)

    -- 除了 data_plane 的节点，其他 节点都需要初始化 etcd
    if env.deployment_role ~= "data_plane" then
        init_etcd(env, args)
    end

    -- 执行启动命令，其实就是 openresty -p ${apisix_home} -c ${apisix_home}/conf/nginx.conf
    util.execute_cmd(env.openresty_args)
end
```



说明：

- APISIX 针对不同的生产用例有三种不同的**部署模式**：
  - `traditional` 模式，在这个模式下 APISIX 的控制平台和数据平面在一起
  - `decoupled` 模式，数据平面和控制平面分开，控制平面专门作 Admin API
  - `standalone` 模式，只有数据平面，使用 `conf/apisix.yaml` 作为配置文件，并且每间隔一段时间自动检测文件内容是否有更新，如果有更新则重新加载配置。不过这个模式只能作为数据平面，无法使用 Admin API 等管理功能。也就是通过 yaml 文件定义route、upstream 等信息。
- 执行启动命令，其实就是 openresty -p ${apisix_home} -c ${apisix_home}/conf/nginx.conf



### init 阶段

init 阶段是 apisix 初始化部分，比较重要，内容也比较多，下面针对重要的部分简要说明：

1. **读取并校验配置文件的 schema**
2. **校验 deployment.admin.admin_key**
3. **检查 openresty 的版本**，最新的 apisix 需要 `1.21.4` openresty 以上。并且检查 openresty 是否编译进了 **http_stub_status_module** 模块，该模块是输出 nginx 的状态和连接数，监控需要
4. **检查需要开启的子系统**，apisix 默认只开启 http 子系统，如果要开启 stream，需要设置 proxy_mode 为 `stream` 或 `http&stream`
5. **统计默认开启的插件**
   - 如果开启 proxy-cache 插件，必须配置 apisix.proxy_cache
   - 如果开启 batch-requests 插件，批处理插件，会添加一个 admin api，必须确保在 nginx.conf 中没有被用户禁用 real_ip_from 
6. **生成监听 url**，即 `host:port`，有以下几个服务
   - `admin_server`，提供 admin API ，默认 0.0.0.0:9180
   - `control_server`，提供 admin API ，默认 0.0.0.0:9092
   - `prometheus_server`，提供 prometheus 的指标指数，默认 0.0.0.0:9091
   - `apisix_server`，提供路由转发，默认 0.0.0.0:9080，可以配置多个路由转发 server
7. **ssl 配置**，默认使用 9443 端口，检查证书文件是否存在
   - 如果开启了 stream 子系统的 ssl，需要配置 apisix.stream_proxy.tcp
8. **检查 plugin_attr 的配置**，针对下面几个插件，作了特殊的检查
   - dubbo-proxy - 转发请求到上游 dubbo 中
   - proxy-mirror - 镜像客户端请求
   - opentelemetry - 可用于根据  OpenTelemetry specification 协议规范上报 Tracing 数据
   - zipkin - 分布调用链追踪系统
9. **配置参数准备**
10. 如果使用了 **kubernetes 作为服务发现**，将 kubernetes 的配置文件相关内容，注入到配置参数中。
11. 根据 apisix/cli/ngx_tpl.lua 配置的 nginx.conf 的模板文件，**生成 nginx.conf 文件**





### etcd_init

etcd_init 阶段相对比较简单，就是读取 config.yaml 配置文件并解析，如果 config_provider 是 etcd，那么就根据 etcd.host 的配置，通过 http 请求获取 etcd 的 version 信息。

apisix 使用的 http stream 的形式与 etcd 通信，而不是 rpc 的形式。







## Apisix 启动后 lua 基本的执行



在  `openresty -p ${apisix_home} -c ${apisix_home}/conf/nginx.conf` 命令执行启动 nginx 后，通过 nginx 的配置文件，可以看到各个阶段执行的 lua 脚本，首先，在 init 阶段。

```nginx
    init_by_lua_block {
        require "resty.core"
        apisix = require("apisix")

        local dns_resolver = { "127.0.0.11", }
        local args = {
            dns_resolver = dns_resolver,
        }
        apisix.http_init(args)

        -- set apisix_lua_home into constants module
        -- it may be used by plugins to determine the work path of apisix
        local constants = require("apisix.constants")
        constants.apisix_lua_home = "/usr/local/apisix"
    }

    init_worker_by_lua_block {
        apisix.http_init_worker()
    }

    exit_worker_by_lua_block {
        apisix.http_exit_worker()
    }

```

说明：

- 在 master 启动过程中，执行 `init_by_lua_block` 块下的 lua 脚本，主要是执行了 http_init 方法
- 在 worker 启动过程中，执行 `init_worker_by_lua_block`  块下的 lua 脚本，执行了 http_init_worker 方法
- 在 worker 退出过程中，执行 `exit_worker_by_lua_block`  块下的 lua 脚本，执行了 http_exit_worker 方法



### 1. init_by_lua_block

 http_init 方法在 `apisix/init.lua` :

```lua
local config_provider = local_conf.deployment and local_conf.deployment.config_provider
                      or "etcd"
log.info("use config_provider: ", config_provider)
local config = require("apisix.core.config_" .. config_provider)

....

function _M.http_init(args)
    -- 如果传入了 dns，加入 /etc/hosts
    core.resolver.init_resolver(args)
    -- 初始化 instance id，pid
    core.id.init()
    -- 获取 系统环境变量，存入 table
    core.env.init()

	-- 开启特权级特权进程
    local process = require("ngx.process")
    local ok, err = process.enable_privileged_agent()
    if not ok then
        core.log.error("failed to enable privileged_agent: ", err)
    end

    # 关键，执行 config_etcd 的初始化
    if core.config.init then
        local ok, err = core.config.init()
        if not ok then
            core.log.error("failed to load the configuration: ", err)
        end
    end

    xrpc.init()
end
```

说明：

- 特权进程只能在 init_by_lua 上下文中开启
- 特权进程拥有和 master 进程一样的权限，一般来说是 root 用户的权限，这就让它可以做很多 worker 进程不可能完成的任务；
- config_provider 默认是 etcd，因此 `core.config.init` 就是 `core.config_etcd.init`
  - 在 core.config_etcd.init 方法中，作用是初始化一个 etcd client，并检查 etcd 集群是否 health
  - etcd 存储的 apisix 相关内容，都以 /apisix 为前缀
  - 尝试从 etcd 中获取所有 `preifx = /apisix` 的数据，验证 etcd 是否可用
- Apache APISIX 实现了一个 L4 协议扩展框架 xRPC，允许开发者定制应用特定的协议。
  - xRPC 是一个基础框架，而不是一种具体协议的实现
  - 根据 xRPC 框架，目前已经支持了 go/python/java 三种 插件的 Runner，参考 [多协议接入框架 xRPC 细节前瞻](https://apisix.apache.org/zh/blog/2022/01/21/apisix-xrpc-details-and-miltilingual/)
  - `xrpc.init` 在 admin api 中注册了`apisix.stream.xrpc.protocols.[NAME].schema`，其中具体的协议需要自己实现，目前有 redis 和 dubbo



### 2. init_worker_by_lua_block

http_init_worker 方法同样在  `apisix/init.lua` :

```lua
function _M.http_init_worker()
    -- 初始化随机数种子
    local seed, err = core.utils.get_seed_from_urandom()
    if not seed then
        core.log.warn('failed to get seed from urandom: ', err)
        seed = ngx_now() * 1000 + ngx.worker.pid()
    end
    math.randomseed(seed)
    -- for testing only
    core.log.info("random test in [1, 10000]: ", math.random(1, 10000))

    -- 初始化 worker events ，用于 worker 进程间通信
    require("apisix.events").init_worker()

    -- 服务发现的初始化
    local discovery = require("apisix.discovery.init").discovery
    if discovery and discovery.init_worker then
        discovery.init_worker()
    end
    
    -- 负载均衡初始化 - 什么也没做
    require("apisix.balancer").init_worker()
    -- 支持的负载均衡算法
    load_balancer = require("apisix.balancer")
    
    -- admin api 路由的初始化，并且注册了 plugin reload event，用于实现插件的热加载
    require("apisix.admin.init").init_worker()

    -- 初始化定时器
    require("apisix.timers").init_worker()

    require("apisix.debug").init_worker()

    -- 初始化 config_etcd ，因为之前在 init_by_lua_block 已经初始化了，这里值时检查了 yaml 文件
    if core.config.init_worker then
        local ok, err = core.config.init_worker()
        if not ok then
            core.log.error("failed to init worker process of ", core.config.type,
                           " config center, err: ", err)
        end
    end

    -- 下面就是在 etcd 中注册 watcher
    -- 在 etcd 注册 /plugins 和 /plugin_metadata 的 watcher
    plugin.init_worker()
    -- 在 etcd 注册 /routes 的 watcher
    router.http_init_worker()
    -- 在 etcd 注册 /services 的 watcher
    require("apisix.http.service").init_worker()
    -- 在 etcd 注册 /plugin_configs 的 watcher
    plugin_config.init_worker()
    -- 在 etcd 注册 /consumers 的 watcher
    require("apisix.consumer").init_worker()
    -- 在 etcd 注册 /consumer_groups 的 watcher
    consumer_group.init_worker()
    -- 在 etcd 注册 /secrets 的 watcher
    apisix_secret.init_worker()

    -- 在 etcd 注册 /global_rules 的 watcher
    apisix_global_rules.init_worker()

    -- 在 etcd 注册 /upstreams 的 watcher
    apisix_upstream.init_worker()
    
    -- ext-plugin 插件初始化，兼容 go/python/java 插件
    require("apisix.plugins.ext-plugin.init").init_worker()

    -- 注册 reload plugin handler
    control_api_router.init_worker()
    local_conf = core.config.local_conf()

    if local_conf.apisix and local_conf.apisix.enable_server_tokens == false then
        ver_header = "APISIX"
    end
end
```

说明：

- openresty 的 evets 库，多个 worker 之间需要一个通知的机制，在共享字典中维护了一个版本号，在有新消息需要发布的时候，给这个版本号加一，并把消息内容放到以版本号为 key 的字典中
- 服务发现的初始化过程：
  - 调用 `apisix/discovery/init.lua`  的 init_worker 函数，通过遍历被置 discovery 下的所有服务发现后端，调用对应后端的 init_worker 方法
  - 例如 config.yaml 中配置了 discovery.eureka ，就会调用 `apisix/discovery/eureka/init.lua` 的 init_worker 函数
  - 原理就是初始化时，启动一个定时器，定时从服务发现后端获取 node 节点信息，存入本地内存中。
- admin API 初始化时，会同时注册  plugin reload event，用于在不同 worker 进程同步，实现插件的热加载
  - 与此同时，作了一次配置同步，本地 config.yaml 文件中的 plugins 配置信息，如果与 etcd 中存储的不同，则同步到 etcd 中
- 初始化定时器，这里启动了一个名为 `background` 的定时任务，默认每隔 1s 执行一次
  -  `background` 的定时任务的作用就是，维护一个全局 timer 变量，遍历注册的定时器，并执行。

- ext-plugin 插件是为了扩展其他语言编写的插件，目前支持的有 python/go/java。
  - 必须在配置文件 config.yaml 中配置  ext-plugin.cmd 插件的启动命令
  - ext-plugin 插件的初始化过程，会把 plugin runner 作为自己的一个子进程，该子进程与 Apache APISIX 进程属于同一个用户，当重启或重新加载 Apache APISIX 时，plugin runner 也将被重启（通过注册的 events）
  - 通过管道，worker 进程与 plugin runner 进行通信

 

### 3. exit_worker_by_lua_block







## etcd 的 watch 机制

[Apache APISIX 架构分析：如何动态管理 Nginx 集群](https://apisix.apache.org/zh/blog/2021/08/10/apisix-nginx/#%E5%9F%BA%E4%BA%8E-etcd-watch-%E6%9C%BA%E5%88%B6%E7%9A%84%E9%85%8D%E7%BD%AE%E5%90%8C%E6%AD%A5%E6%96%B9%E6%A1%88)



归根结底，两个步骤：

1. 通过 admin api 创建相应的 route/plugin/upstream 资源，并且写入到 etcd 中
2. Apisix 通过定时通过 http stream 的方式，获取数据，将对应的资源，放入到内存对应的 table 中。
3. apisix 内部维护了一个 radixtree 的路由表，进行内部的路由转发，可以实现路由的热加载，不需要 reload



Apache APISIX 将需要监控的配置以不同的前缀存入了 etcd，目前包括以下 11 种：

- /apisix/consumers/：Apache APISIX 支持以 consumer 抽象上游种类
- /apisix/global_rules/：全局通用的规则
- /apisix/plugin_configs/：可以在不同 Router 间复用的 Plugin
- /apisix/plugin_metadata/：部分插件的元数据
- /apisix/plugins/：所有 Plugin 插件的列表
- /apisix/proto/：当透传 gRPC 协议时，部分插件需要转换协议内容，该配置存储 protobuf 消息定义
- /apisix/routes/：路由信息，是 HTTP 请求匹配的入口，可以直接指定上游 Server，也可以挂载 services 或者 upstream
- /apisix/services/：可以将相似的 router 中的共性部分抽象为 services，再挂载 plugin
- /apisix/ssl/：SSL 证书公、私钥及相关匹配规则
- /apisix/stream_routes/：OSI 四层网关的路由匹配规则
- /apisix/upstreams/：对一组上游 Server 主机的抽象





